---
layout: amt-post 
title: "Streaming with Apache Storm"
location: Helsinki
tags: [Streaming]
---

TBD and WIP. Previous blog posts briefly demonstrate some of my experiences on streaming data processing using mostly Kafka and Spark. Still we need to remember that we have realtime processing capability via Kafka and near-realtime processing capability using Spark. This is mostly due to the fact that Spark processes a stream of RDDs generated by some time window. In this article, let me quickly walk through some basic idea and example of streaming data processing using Apache Storm which is another popular streaming processing framework. Sadly, storm is not part of Cloudera package. So if you have a sandbox with Cloudera and you will be missing this storm. 

# Related articles

1. [Streaming processing (III): Best Spark Practice](/amt/streaming-processing-iii.html)
1. [Streaming processing (II):  Best Kafka Practice](/amt/streaming-processing-ii.html)
1. [Streaming processing (I):   Kafka, Spark, Avro Integration](/amt/spark-streaming-kafka-avro-and-registry.html)

# Table of content
* auto-gen TOC:
{:toc}

# Package and versions

The following packages are required.

| Packages           | Version  | Repository                                 |
|:-------------------|---------:|-------------------------------------------:|
| mvn                | 3.3.9    |                                            | 
| gradle             | 3.3      |                                            |
| storm              | 1.0.3    | git@github.com:hongyusu/storm.git          |


# Storm build and installation

1. Download the source code of *storm* from [repo](git@github.com:hongyusu/storm.git), enter the source directory, and build the code with *maven* 

   ```shell
   mvn clean install -DskipTests=true
   cd storm-dist/binary/
   mvn package -Dgpg.skip -Dtest.skip
   ```

1. Find the release package and unpack the tar ball 

   ```shell
   cd target
   tar xvvf apache-storm-1.0.3.tar.gz
   ```

1. Run from the *bin/* folder inside the release package and you are ready to go.


# Build streaming workflow with storm

Storm seems to have a pretty rich collection of APIs for connecting sources and processing streaming data. In addition, there is a so called _trident_ API that provides, in my opinion APIs.
I will make two separate examples in the following subsections using storm native APIs and trident APIs.
Again source code can be found from [my git:bigdata:storm][stormpackage].

## Storm native API

## Storm trident API


# Conclusion



    

[stormpackage]: https://github.com/hongyusu/bigdata_etl/tree/master/etl_spark
[stormkbuild]:   https://github.com/hongyusu/bigdata_etl/blob/master/etl_storm/build.gradle




